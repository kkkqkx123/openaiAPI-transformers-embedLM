# 需求：OpenAI 兼容的嵌入模型 API

## 简介

本项目旨在创建一个使用 `transformers` 库和 `all-miniLM-L12-v2` 模型的嵌入服务。该服务将通过 FastAPI 部署，并提供与 OpenAI `embeddings` API 兼容的接口。此外，系统还将包含一个管理模块，用于配置和控制关键参数，如内存占用、批处理大小和最大上下文长度。

## 1. 核心功能

### 1.1 用户故事
**作为一名开发者**，我希望能够通过一个与 OpenAI `embeddings` API 兼容的接口，轻松地将文本转换为高质量的嵌入向量，以便我可以在我的应用程序中无缝替换或集成此服务。

### 1.2. 验收标准
- **UB-1.1**: 当系统收到一个有效的 `/v1/embeddings` POST 请求时，它必须返回一个包含嵌入向量的 JSON 对象，其结构与 OpenAI `embeddings` API 的响应一致。
- **UB-1.2**: 当系统在 `D:\models\all-MiniLM-L12-v2` 路径下找不到预训练模型时，它必须尝试从 Hugging Face Hub 下载并保存到该指定目录。
- **UB-1.3**: 当系统在 `D:\models\all-MiniLM-L12-v2` 路径下找到预训练模型时，它必须直接从本地加载该模型，而不是重新下载。
- **UB-1.4**: 当向 `/v1/models` 发送 GET 请求时，系统必须返回一个模型列表，其中包括 `all-miniLM-L12-v2`。

## 2. 管理与配置

### 2.1. 用户故事
**作为一名系统管理员**，我希望能够配置服务的运行参数，如内存占用限制、批处理大小和最大上下文长度，以优化资源使用并确保服务的稳定性。

### 2.2. 验收标准
- **UB-2.1**: 系统必须提供一种机制（例如，配置文件或环境变量），允许管理员设置最大批处理大小。
- **UB-2.2**: 系统必须提供一种机制，允许管理员设置模型处理的最大上下文长度。
- **UB-2.3**: 系统必须提供一种机制，允许管理员配置内存使用限制，以防止服务消耗过多资源。
- **UB-2.4**: 如果请求超出了配置的限制（例如，批处理大小或上下文长度），系统必须返回一个清晰的错误消息。

## 3. 日志功能

### 3.1. 用户故事
**作为一名系统管理员或开发者**，我希望能够查看服务的运行日志，以便监控服务状态、排查问题和分析使用情况。

### 3.2. 验收标准
- **UB-3.1**: 系统必须记录所有传入的 API 请求，包括请求路径、方法和时间戳。
- **UB-3.2**: 系统必须记录所有 API 响应，包括状态码和响应时间。请求、响应的详细内容仅在DEBUG模式下记录。
- **UB-3.3**: 系统必须记录所有错误和异常，包括详细的错误堆栈信息。
- **UB-3.4**: 系统必须记录模型加载事件（例如，从本地加载或从远程下载）。
- **UB-3.5**: 系统必须提供一种机制（例如，配置文件或环境变量）来配置日志级别（如 DEBUG, INFO, WARNING, ERROR）。
- **UB-3.6**: 系统必须支持不同日志级别，包括DEBUG, INFO, WARNING, ERROR。DEBUG以外的日志级别不包含任何文本量过大、过于细节的内容。
- **UB-3.7**: 系统必须将日志输出到标准输出，以便与容器化环境（如 Docker）兼容。
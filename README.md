# Embedding Model Provider API

ä¸€ä¸ªåŸºäº FastAPI çš„ OpenAI å…¼å®¹åµŒå…¥æ¨¡å‹ API æœåŠ¡ï¼Œä½¿ç”¨ `all-MiniLM-L12-v2` æ¨¡å‹æä¾›é«˜è´¨é‡çš„æ–‡æœ¬åµŒå…¥å‘é‡ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**: åŸºäº FastAPI å’Œ PyTorch çš„é«˜æ•ˆæ¨ç†
- ğŸ”„ **OpenAI å…¼å®¹**: å®Œå…¨å…¼å®¹ OpenAI embeddings API æ ¼å¼
- ğŸ“¦ **è‡ªåŠ¨æ¨¡å‹ç®¡ç†**: æ”¯æŒæœ¬åœ°æ¨¡å‹åŠ è½½å’Œä» Hugging Face Hub è‡ªåŠ¨ä¸‹è½½
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**: å®Œå–„çš„é”™è¯¯å¤„ç†æœºåˆ¶ï¼Œéµå¾ª OpenAI API é”™è¯¯æ ¼å¼
- ğŸ“Š **ç»“æ„åŒ–æ—¥å¿—**: JSON æ ¼å¼çš„ç»“æ„åŒ–æ—¥å¿—è¾“å‡º
- âš™ï¸ **çµæ´»é…ç½®**: æ”¯æŒç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶çš„çµæ´»é…ç½®
- ğŸ§ª **å…¨é¢æµ‹è¯•**: åŒ…å«å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•å’Œå…¼å®¹æ€§æµ‹è¯•

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.10 æˆ–æ›´é«˜ç‰ˆæœ¬
- uv åŒ…ç®¡ç†å™¨ï¼ˆæ¨èï¼‰æˆ– pip
- è‡³å°‘ 2GB å¯ç”¨å†…å­˜

### å®‰è£…

1. å…‹éš†ä»“åº“ï¼š
```bash
git clone https://github.com/example/emb-model-provider.git
cd emb-model-provider
```

2. ä½¿ç”¨ uv å®‰è£…ä¾èµ–ï¼ˆæ¨èï¼‰ï¼š
```bash
uv sync
```

æˆ–ä½¿ç”¨ pip å®‰è£…ï¼š
```bash
pip install -e .
```

3. æ¨¡å‹ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°é»˜è®¤è·¯å¾„ `D:\models\all-MiniLM-L12-v2`ï¼ˆWindowsï¼‰æˆ– `/models/all-MiniLM-L12-v2`ï¼ˆLinux/Macï¼‰ã€‚

### è¿è¡ŒæœåŠ¡

1. å¯åŠ¨æœåŠ¡ï¼š
```bash
uv run python -m emb_model_provider.main
```

æˆ–ä½¿ç”¨ uvicornï¼š
```bash
uvicorn emb_model_provider.main:app --host localhost --port 9000
```

2. æœåŠ¡å°†åœ¨ `http://localhost:9000` å¯åŠ¨ã€‚

3. è®¿é—® API æ–‡æ¡£ï¼š
   - Swagger UI: `http://localhost:9000/docs`
   - ReDoc: `http://localhost:9000/redoc`

## ä½¿ç”¨ç¤ºä¾‹

### ä½¿ç”¨ curl

```bash
# è·å–å¯ç”¨æ¨¡å‹
curl http://localhost:9000/v1/models

# åˆ›å»ºåµŒå…¥å‘é‡
curl -X POST "http://localhost:9000/v1/embeddings" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Hello, world!",
    "model": "all-MiniLM-L12-v2"
  }'
```

### ä½¿ç”¨ Python requests

```python
import requests

# è·å–å¯ç”¨æ¨¡å‹
models_response = requests.get("http://localhost:9000/v1/models")
models = models_response.json()
print(models)

# åˆ›å»ºåµŒå…¥å‘é‡
embeddings_response = requests.post(
    "http://localhost:9000/v1/embeddings",
    json={
        "input": "Hello, world!",
        "model": "all-MiniLM-L12-v2"
    }
)
embeddings = embeddings_response.json()
print(embeddings)
```

### ä½¿ç”¨ OpenAI Python å®¢æˆ·ç«¯

```python
from openai import OpenAI

# é…ç½®å®¢æˆ·ç«¯ä½¿ç”¨æœ¬åœ° API
client = OpenAI(
    api_key="dummy-key",  # ä¸éœ€è¦çœŸå®å¯†é’¥
    base_url="http://localhost:9000/v1"
)

# åˆ›å»ºåµŒå…¥å‘é‡
response = client.embeddings.create(
    model="all-MiniLM-L12-v2",
    input="Hello, world!"
)

print(response.data[0].embedding)
```

### æ‰¹é‡å¤„ç†

```python
import requests

# æ‰¹é‡åˆ›å»ºåµŒå…¥å‘é‡
response = requests.post(
    "http://localhost:9000/v1/embeddings",
    json={
        "input": [
            "First sentence",
            "Second sentence",
            "Third sentence"
        ],
        "model": "all-MiniLM-L12-v2"
    }
)

data = response.json()
for i, embedding_data in enumerate(data["data"]):
    print(f"Sentence {i}: {embedding_data['embedding'][:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ªç»´åº¦
```

## é…ç½®

### ç¯å¢ƒå˜é‡

å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®æœåŠ¡ï¼š

```bash
# æ¨¡å‹é…ç½®
export EMB_PROVIDER_MODEL_PATH="/path/to/model"
export EMB_PROVIDER_MODEL_NAME="all-MiniLM-L12-v2"

# å¤„ç†é…ç½®
export EMB_PROVIDER_MAX_BATCH_SIZE=32
export EMB_PROVIDER_MAX_CONTEXT_LENGTH=512
export EMB_PROVIDER_EMBEDDING_DIMENSION=384

# èµ„æºé…ç½®
export EMB_PROVIDER_MEMORY_LIMIT="2GB"
export EMB_PROVIDER_DEVICE="auto"  # auto, cpu, cuda

# API é…ç½®
export EMB_PROVIDER_HOST="localhost"
export EMB_PROVIDER_PORT=9000

# æ—¥å¿—é…ç½®
export EMB_PROVIDER_LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR
```

### é…ç½®æ–‡ä»¶

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# æ¨¡å‹é…ç½®
EMB_PROVIDER_MODEL_PATH=D:\models\all-MiniLM-L12-v2
EMB_PROVIDER_MODEL_NAME=all-MiniLM-L12-v2

# å¤„ç†é…ç½®
EMB_PROVIDER_MAX_BATCH_SIZE=32
EMB_PROVIDER_MAX_CONTEXT_LENGTH=512
EMB_PROVIDER_EMBEDDING_DIMENSION=384

# èµ„æºé…ç½®
EMB_PROVIDER_MEMORY_LIMIT=2GB
EMB_PROVIDER_DEVICE=auto

# API é…ç½®
EMB_PROVIDER_HOST=localhost
EMB_PROVIDER_PORT=9000

# æ—¥å¿—é…ç½®
EMB_PROVIDER_LOG_LEVEL=INFO
```

## API å‚è€ƒ

### ç«¯ç‚¹

#### `GET /v1/models`

åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ã€‚

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "object": "list",
  "data": [
    {
      "id": "all-MiniLM-L12-v2",
      "object": "model",
      "created": 1677610602,
      "owned_by": "organization-owner"
    }
  ]
}
```

#### `POST /v1/embeddings`

ä¸ºç»™å®šçš„è¾“å…¥æ–‡æœ¬åˆ›å»ºåµŒå…¥å‘é‡ã€‚

**è¯·æ±‚ä½“ï¼š**
```json
{
  "input": "Your text here",
  "model": "all-MiniLM-L12-v2",
  "encoding_format": "float",
  "user": "optional-user-id"
}
```

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "object": "list",
  "data": [
    {
      "object": "embedding",
      "embedding": [0.1, 0.2, 0.3, ...],
      "index": 0
    }
  ],
  "model": "all-MiniLM-L12-v2",
  "usage": {
    "prompt_tokens": 5,
    "total_tokens": 5
  }
}
```

#### `GET /health`

å¥åº·æ£€æŸ¥ç«¯ç‚¹ã€‚

**å“åº”ç¤ºä¾‹ï¼š**
```json
{
  "status": "healthy"
}
```

### é”™è¯¯å¤„ç†

API éµå¾ª OpenAI çš„é”™è¯¯å“åº”æ ¼å¼ï¼š

```json
{
  "error": {
    "message": "Error description",
    "type": "error_type",
    "param": "parameter_name",
    "code": "error_code"
  }
}
```

å¸¸è§é”™è¯¯ç±»å‹ï¼š
- `invalid_request_error`: è¯·æ±‚æ— æ•ˆï¼ˆ400ï¼‰
- `context_length_exceeded`: ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™ï¼ˆ429ï¼‰
- `batch_size_exceeded`: æ‰¹å¤„ç†å¤§å°è¶…é™ï¼ˆ429ï¼‰
- `model_not_found`: æ¨¡å‹æœªæ‰¾åˆ°ï¼ˆ404ï¼‰
- `internal_server_error`: å†…éƒ¨æœåŠ¡å™¨é”™è¯¯ï¼ˆ500ï¼‰

## æ€§èƒ½ä¼˜åŒ–

### æ‰¹å¤„ç†

ä½¿ç”¨æ‰¹é‡è¯·æ±‚å¯ä»¥æé«˜ååé‡ï¼š

```python
# ä¸æ¨èï¼šå¤šä¸ªå•ç‹¬è¯·æ±‚
for text in texts:
    response = requests.post("/v1/embeddings", json={"input": text, "model": "all-MiniLM-L12-v2"})

# æ¨èï¼šå•ä¸ªæ‰¹é‡è¯·æ±‚
response = requests.post("/v1/embeddings", json={"input": texts, "model": "all-MiniLM-L12-v2"})
```

### é…ç½®è°ƒä¼˜

æ ¹æ®ç¡¬ä»¶èµ„æºè°ƒæ•´é…ç½®ï¼š

- **CPU ç¯å¢ƒ**: è®¾ç½® `EMB_PROVIDER_DEVICE=cpu`ï¼Œå‡å° `EMB_PROVIDER_MAX_BATCH_SIZE`
- **GPU ç¯å¢ƒ**: è®¾ç½® `EMB_PROVIDER_DEVICE=cuda`ï¼Œå¢å¤§ `EMB_PROVIDER_MAX_BATCH_SIZE`
- **å†…å­˜å—é™**: å‡å° `EMB_PROVIDER_MAX_BATCH_SIZE` å’Œ `EMB_PROVIDER_MEMORY_LIMIT`
ç›´æ¥ä½¿ç”¨autoä¹Ÿå¯ä»¥

## å¼€å‘

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
uv run pytest

# è¿è¡Œç‰¹å®šæµ‹è¯•
uv run pytest tests/test_e2e.py
uv run pytest tests/test_performance.py
uv run pytest tests/test_openai_compatibility.py

# è¿è¡Œæµ‹è¯•å¹¶ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
uv run pytest --cov=emb_model_provider
```

### ä»£ç æ ¼å¼åŒ–

```bash
# æ ¼å¼åŒ–ä»£ç 
uv run black .
uv run isort .

# æ£€æŸ¥ä»£ç è´¨é‡
uv run flake8 emb_model_provider
uv run mypy emb_model_provider
```

### é¡¹ç›®ç»“æ„

```
emb_model_provider/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py              # FastAPI åº”ç”¨å…¥å£
â”œâ”€â”€ api/                 # API è·¯ç”±
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ embeddings.py    # åµŒå…¥ç«¯ç‚¹
â”‚   â”œâ”€â”€ models.py        # æ¨¡å‹ç«¯ç‚¹
â”‚   â”œâ”€â”€ exceptions.py    # å¼‚å¸¸å®šä¹‰
â”‚   â””â”€â”€ middleware.py    # ä¸­é—´ä»¶
â”œâ”€â”€ core/                # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py        # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ logging.py       # æ—¥å¿—é…ç½®
â”‚   â””â”€â”€ model_manager.py # æ¨¡å‹ç®¡ç†
â””â”€â”€ services/            # æœåŠ¡å±‚
    â”œâ”€â”€ __init__.py
    â””â”€â”€ embedding_service.py # åµŒå…¥æœåŠ¡
```

## Docker éƒ¨ç½²

### æ„å»ºé•œåƒ

```bash
docker build -t emb-model-provider .
```

### è¿è¡Œå®¹å™¨

```bash
docker run -p 9000:9000 \
  -v /path/to/models:/models \
  -e EMB_PROVIDER_MODEL_PATH=/models/all-MiniLM-L12-v2 \
  emb-model-provider
```

### ä½¿ç”¨ Docker Compose

```yaml
version: '3.8'
services:
  emb-model-provider:
    build: .
    ports:
      - "9000:9000"
    volumes:
      - ./models:/models
    environment:
      - EMB_PROVIDER_MODEL_PATH=/models/all-MiniLM-L12-v2
      - EMB_PROVIDER_LOG_LEVEL=INFO
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
   - æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„

2. **å†…å­˜ä¸è¶³**
   - å‡å° `EMB_PROVIDER_MAX_BATCH_SIZE`
   - è®¾ç½® `EMB_PROVIDER_DEVICE=cpu`
   - å¢åŠ  `EMB_PROVIDER_MEMORY_LIMIT`

3. **å“åº”æ—¶é—´æ…¢**
   - ä½¿ç”¨ GPU åŠ é€Ÿï¼š`EMB_PROVIDER_DEVICE=cuda`
   - å¢åŠ æ‰¹å¤„ç†å¤§å°ï¼š`EMB_PROVIDER_MAX_BATCH_SIZE`
   - æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ

### æ—¥å¿—åˆ†æ

å¯ç”¨ DEBUG çº§åˆ«æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ï¼š

```bash
export EMB_PROVIDER_LOG_LEVEL=DEBUG
```

æ—¥å¿—ä»¥ JSON æ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ï¼š
- è¯·æ±‚ ID è·Ÿè¸ª
- æ€§èƒ½æŒ‡æ ‡
- é”™è¯¯è¯¦æƒ…
- æ¨¡å‹åŠ è½½äº‹ä»¶

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·ï¼š

1. æŸ¥çœ‹ [FAQ](#æ•…éšœæ’é™¤)
2. æœç´¢ [Issues](https://github.com/example/emb-model-provider/issues)
3. åˆ›å»ºæ–°çš„ Issue

## æ›´æ–°æ—¥å¿—

### v0.1.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- OpenAI å…¼å®¹çš„ embeddings API
- æ”¯æŒå•ä¸ªå’Œæ‰¹é‡æ–‡æœ¬åµŒå…¥
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- å…¨é¢çš„æµ‹è¯•å¥—ä»¶
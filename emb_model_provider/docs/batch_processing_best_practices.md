# æ‰¹å¤„ç†ä¼˜åŒ–æœ€ä½³å®è·µæŒ‡å—

## ğŸ“‹ é¡¹ç›®ä¿®æ”¹æ€»ç»“

### æ–°å¢æ–‡ä»¶

#### 1. æ ¸å¿ƒæ¨¡å—
- **`emb_model_provider/core/performance_monitor.py`**
  - å®ç°æ€§èƒ½ç›‘æ§åŠŸèƒ½
  - æä¾›æ‰¹å¤„ç†æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æ
  - æ”¯æŒå®æ—¶ç›‘æ§å’ŒæŠ¥å‘Šç”Ÿæˆ

- **`emb_model_provider/core/tokenizer_manager.py`**
  - è§£å†³tokenizerå¹¶å‘è®¿é—®é—®é¢˜
  - å®ç°çº¿ç¨‹æœ¬åœ°å­˜å‚¨å’Œæ± åŒ–ç®¡ç†ç­–ç•¥
  - æä¾›ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºå®‰å…¨

#### 2. æœåŠ¡å±‚
- **`emb_model_provider/services/batch_optimizer.py`**
  - å®ç°æ™ºèƒ½é•¿åº¦åˆ†ç»„ç®—æ³•
  - æä¾›åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–ç­–ç•¥
  - å‡å°‘paddingå¼€é”€ï¼Œæé«˜GPUåˆ©ç”¨ç‡

#### 3. æµ‹è¯•æ–‡ä»¶
- **`tests/test_batch_optimization.py`**
  - æ‰¹å¤„ç†ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•
  - é•¿åº¦åˆ†ç»„å’ŒåŠ¨æ€æ‰¹å¤„ç†æµ‹è¯•

- **`tests/test_tokenizer_manager.py`**
  - tokenizerç®¡ç†å™¨çº¿ç¨‹å®‰å…¨æµ‹è¯•
  - å¹¶å‘è®¿é—®æµ‹è¯•

- **`tests/test_performance_comparison.py`**
  - æ€§èƒ½å¯¹æ¯”æµ‹è¯•
  - ä¼˜åŒ–æ•ˆæœéªŒè¯

#### 4. æ–‡æ¡£
- **`emb_model_provider/docs/batch_optimization_summary.md`**
  - è¯¦ç»†çš„ä¼˜åŒ–åˆ†ææŠ¥å‘Š
  - æŠ€æœ¯å®ç°ç»†èŠ‚å’Œæ€§èƒ½æ•°æ®

### ä¿®æ”¹æ–‡ä»¶

#### 1. é…ç½®ç³»ç»Ÿ (`emb_model_provider/core/config.py`)
```python
# æ–°å¢é…ç½®å‚æ•°
max_batch_size: int = Field(default=32, ge=1, le=512)  # æé«˜é™åˆ¶
enable_dynamic_batching: bool = Field(default=True)
max_wait_time_ms: int = Field(default=100)
min_batch_size: int = Field(default=1)
enable_length_grouping: bool = Field(default=True)
length_group_tolerance: float = Field(default=0.2)

# æ–°å¢æ–¹æ³•
def get_optimal_batch_size(self) -> int
def optimize_for_hardware(self) -> None
```

#### 2. åµŒå…¥æœåŠ¡ (`emb_model_provider/services/embedding_service.py`)
```python
# æ›¿æ¢çº¿ç¨‹é”ä¸ºtokenizerç®¡ç†å™¨
self.tokenizer_manager = initialize_tokenizer_manager(
    config.model_path,
    use_thread_local=True,
    pool_size=4
)

# ä¼˜åŒ–æ‰¹å¤„ç†æµç¨‹
def generate_embeddings(self, inputs: List[str]) -> List[EmbeddingData]
def _process_batch_group(self, group: BatchGroup) -> List[List[float]]
```

#### 3. APIå±‚ (`emb_model_provider/api/embeddings.py`)
```python
# æ–°å¢æ€§èƒ½ç›‘æ§API
@router.get("/v1/performance")
@router.post("/v1/performance/reset")
```

#### 4. ç¯å¢ƒé…ç½® (`.env` å’Œ `.env.example`)
```bash
# æ‰¹å¤„ç†ä¼˜åŒ–é…ç½®
EMB_PROVIDER_ENABLE_DYNAMIC_BATCHING=true
EMB_PROVIDER_MAX_WAIT_TIME_MS=100
EMB_PROVIDER_MIN_BATCH_SIZE=1
EMB_PROVIDER_ENABLE_LENGTH_GROUPING=true
EMB_PROVIDER_LENGTH_GROUP_TOLERANCE=0.2
```

## ğŸš€ æ‰¹å¤„ç†ç¯å¢ƒæœ€ä½³å®è·µ

### 1. æ¶æ„è®¾è®¡åŸåˆ™

#### çœŸæ­£åˆ©ç”¨æ¨¡å‹æ‰¹å¤„ç†èƒ½åŠ›
```python
# âœ… æ­£ç¡®åšæ³•ï¼šçœŸæ­£çš„æ‰¹å¤„ç†
encoded_inputs = tokenizer(
    inputs,  # æ‰¹é‡è¾“å…¥
    padding=True,
    truncation=True,
    return_tensors='pt'
)

with torch.no_grad():
    model_output = model(**encoded_inputs)  # ä¸€æ¬¡å‰å‘ä¼ æ’­å¤„ç†æ‰€æœ‰è¾“å…¥
```

#### é¿å…ä¼ªè£…æ‰¹å¤„ç†
```python
# âŒ é”™è¯¯åšæ³•ï¼šä¼ªè£…æ‰¹å¤„ç†
embeddings = []
for input_text in inputs:
    embedding = model.encode(input_text)  # é€ä¸ªå¤„ç†
    embeddings.append(embedding)
```

### 2. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### æ™ºèƒ½é•¿åº¦åˆ†ç»„
```python
def optimize_batch_processing(self, inputs: List[str]):
    """æŒ‰é•¿åº¦åˆ†ç»„å‡å°‘paddingå¼€é”€"""
    # 1. è®¡ç®—æ¯ä¸ªè¾“å…¥çš„tokené•¿åº¦
    text_lengths = [(i, text, len(tokenizer.encode(text))) for i, text in enumerate(inputs)]
    
    # 2. æŒ‰é•¿åº¦æ’åº
    text_lengths.sort(key=lambda x: x[2])
    
    # 3. åˆ†ç»„ï¼šæ¯ç»„é•¿åº¦å·®å¼‚ä¸è¶…è¿‡å®¹å¿åº¦
    groups = self._group_by_length(text_lengths, tolerance=0.2)
    
    return groups
```

#### ç¡¬ä»¶è‡ªé€‚åº”é…ç½®
```python
def optimize_for_hardware(self):
    """æ ¹æ®ç¡¬ä»¶ç‰¹æ€§ä¼˜åŒ–é…ç½®"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory
        
        if gpu_memory >= 24 * 1024**3:  # RTX 4090, A100
            self.max_batch_size = 256
        elif gpu_memory >= 16 * 1024**3:  # RTX 3090, V100
            self.max_batch_size = 128
        else:
            self.max_batch_size = 64
```

### 3. å¹¶å‘å¤„ç†æœ€ä½³å®è·µ

#### Tokenizerçº¿ç¨‹å®‰å…¨
```python
# âœ… æ¨èåšæ³•ï¼šçº¿ç¨‹æœ¬åœ°å­˜å‚¨
class ThreadSafeTokenizerManager:
    def __init__(self, model_path: str):
        self._master_tokenizer = AutoTokenizer.from_pretrained(model_path)
        self._thread_local = threading.local()
    
    def get_tokenizer(self):
        if not hasattr(self._thread_local, 'tokenizer'):
            self._thread_local.tokenizer = copy.deepcopy(self._master_tokenizer)
        return self._thread_local.tokenizer

# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with tokenizer_manager.get_tokenizer_context() as tokenizer:
    encoded_inputs = tokenizer(inputs, padding=True, return_tensors='pt')
```

#### ç¯å¢ƒå˜é‡é…ç½®
```bash
# å¿…é¡»è®¾ç½®ä»¥é¿å…tokenizerå¹¶è¡Œæ€§è­¦å‘Š
export TOKENIZERS_PARALLELISM=false
```

### 4. å†…å­˜ç®¡ç†ä¼˜åŒ–

#### å‡å°‘Paddingå¼€é”€
```python
# âœ… ä¼˜åŒ–å‰ï¼šæ‰€æœ‰æ–‡æœ¬å¡«å……åˆ°æœ€é•¿é•¿åº¦
max_length = max(len(text.split()) for text in inputs)
encoded = tokenizer(inputs, max_length=max_length, padding=True)

# âœ… ä¼˜åŒ–åï¼šæŒ‰é•¿åº¦åˆ†ç»„ï¼Œå‡å°‘padding
groups = group_by_length(inputs, tolerance=0.2)
for group in groups:
    group_max_length = max(len(text.split()) for text in group)
    encoded = tokenizer(group, max_length=group_max_length, padding=True)
```

#### GPUå†…å­˜ç®¡ç†
```python
# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨
with torch.no_grad():  # æ¨ç†æ—¶ç¦ç”¨æ¢¯åº¦
    model_output = model(**encoded_inputs)

# åŠæ—¶é‡Šæ”¾GPUå†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

### 5. ç›‘æ§å’Œè°ƒè¯•

#### æ€§èƒ½ç›‘æ§
```python
@contextmanager
def monitor_request(batch_size: int, inputs: List[str]):
    start_time = time.time()
    start_memory = get_gpu_memory_usage()
    
    try:
        yield
    finally:
        end_time = time.time()
        end_memory = get_gpu_memory_usage()
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        metrics = {
            'batch_size': batch_size,
            'processing_time': end_time - start_time,
            'memory_delta': end_memory - start_memory,
            'padding_ratio': calculate_padding_ratio(inputs)
        }
```

#### å…³é”®æ€§èƒ½æŒ‡æ ‡
- **ååé‡**ï¼šæ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°
- **å»¶è¿Ÿ**ï¼šå•ä¸ªè¯·æ±‚çš„å¤„ç†æ—¶é—´
- **GPUåˆ©ç”¨ç‡**ï¼šGPUè®¡ç®—èµ„æºçš„ä½¿ç”¨æ•ˆç‡
- **å†…å­˜æ•ˆç‡**ï¼šGPUå†…å­˜çš„ä½¿ç”¨æƒ…å†µ
- **Paddingæ•ˆç‡**ï¼šæœ‰æ•ˆtokenå æ€»tokençš„æ¯”ä¾‹

### 6. é…ç½®ä¼˜åŒ–å»ºè®®

#### ä¸åŒç¡¬ä»¶é…ç½®

**é«˜ç«¯GPU (RTX 4090, A100)**
```bash
EMB_PROVIDER_MAX_BATCH_SIZE=256
EMB_PROVIDER_MAX_WAIT_TIME_MS=150
EMB_PROVIDER_MIN_BATCH_SIZE=4
EMB_PROVIDER_DEVICE=cuda
```

**ä¸­ç«¯GPU (RTX 3060, 3080)**
```bash
EMB_PROVIDER_MAX_BATCH_SIZE=128
EMB_PROVIDER_MAX_WAIT_TIME_MS=100
EMB_PROVIDER_MIN_BATCH_SIZE=2
EMB_PROVIDER_DEVICE=cuda
```

**CPUç¯å¢ƒ**
```bash
EMB_PROVIDER_MAX_BATCH_SIZE=32
EMB_PROVIDER_MAX_WAIT_TIME_MS=50
EMB_PROVIDER_MIN_BATCH_SIZE=1
EMB_PROVIDER_DEVICE=cpu
```

#### ä¸åŒè´Ÿè½½åœºæ™¯

**é«˜ååé‡åœºæ™¯**
```bash
EMB_PROVIDER_ENABLE_DYNAMIC_BATCHING=true
EMB_PROVIDER_MAX_WAIT_TIME_MS=200
EMB_PROVIDER_ENABLE_LENGTH_GROUPING=true
```

**ä½å»¶è¿Ÿåœºæ™¯**
```bash
EMB_PROVIDER_ENABLE_DYNAMIC_BATCHING=false
EMB_PROVIDER_MAX_BATCH_SIZE=8
EMB_PROVIDER_MIN_BATCH_SIZE=1
```

### 7. é”™è¯¯å¤„ç†å’Œå®¹é”™

#### æ‰¹å¤„ç†å¤§å°é™åˆ¶
```python
def validate_batch_size(self, inputs: List[str]):
    if len(inputs) > self.config.max_batch_size:
        raise BatchSizeExceededError(
            max_size=self.config.max_batch_size,
            actual_size=len(inputs)
        )
```

#### ä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥
```python
def validate_context_length(self, inputs: List[str]):
    for text in inputs:
        tokens = self.tokenizer.encode(text, add_special_tokens=True)
        if len(tokens) > self.config.max_context_length:
            raise ContextLengthExceededError(
                max_length=self.config.max_context_length,
                actual_length=len(tokens)
            )
```

### 8. æµ‹è¯•å’ŒéªŒè¯

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
def benchmark_batch_processing():
    test_sizes = [1, 4, 8, 16, 32, 64, 128]
    results = {}
    
    for batch_size in test_sizes:
        inputs = [f"Test text {i}" for i in range(batch_size)]
        
        start_time = time.time()
        response = embedding_service.process_embedding_request(
            EmbeddingRequest(input=inputs, model="test-model")
        )
        end_time = time.time()
        
        results[batch_size] = {
            'throughput': batch_size / (end_time - start_time),
            'latency': end_time - start_time
        }
    
    return results
```

#### å¹¶å‘æµ‹è¯•
```python
def test_concurrent_access():
    def worker():
        response = client.post("/v1/embeddings", json=test_request)
        return response.json()
    
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = [executor.submit(worker) for _ in range(100)]
        results = [future.result() for future in futures]
    
    # éªŒè¯æ‰€æœ‰è¯·æ±‚éƒ½æˆåŠŸ
    assert all(result.get('data') for result in results)
```

## ğŸ¯ å…³é”®æˆåŠŸæŒ‡æ ‡

### æ€§èƒ½æŒ‡æ ‡
- **æ‰¹å¤„ç†æ•ˆç‡æå‡**ï¼š30-50%
- **Paddingæ•ˆç‡**ï¼š>95%
- **å¹¶å‘å¤„ç†èƒ½åŠ›**ï¼šæ”¯æŒ20+å¹¶å‘è¯·æ±‚
- **å†…å­˜åˆ©ç”¨ç‡**ï¼šGPUå†…å­˜ä½¿ç”¨ç‡<80%

### è´¨é‡æŒ‡æ ‡
- **æµ‹è¯•è¦†ç›–ç‡**ï¼š>95%
- **é”™è¯¯ç‡**ï¼š<0.1%
- **å¯ç”¨æ€§**ï¼š99.9%
- **å‘åå…¼å®¹æ€§**ï¼š100%

### è¿ç»´æŒ‡æ ‡
- **ç›‘æ§è¦†ç›–ç‡**ï¼š100%
- **å‘Šè­¦å“åº”æ—¶é—´**ï¼š<5åˆ†é’Ÿ
- **æ•…éšœæ¢å¤æ—¶é—´**ï¼š<10åˆ†é’Ÿ
- **éƒ¨ç½²æˆåŠŸç‡**ï¼š>99%

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- [PyTorchæ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [CUDAç¼–ç¨‹æœ€ä½³å®è·µ](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

### ç›¸å…³ç ”ç©¶
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)

### ç¤¾åŒºèµ„æº
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [PyTorch Forums](https://discuss.pytorch.org/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/pytorch+huggingface)

---

è¿™ä»½æœ€ä½³å®è·µæŒ‡å—æ€»ç»“äº†æœ¬é¡¹ç›®åœ¨æ‰¹å¤„ç†ä¼˜åŒ–æ–¹é¢çš„æ‰€æœ‰ç»éªŒå’Œæ•™è®­ï¼Œä¸ºç±»ä¼¼é¡¹ç›®æä¾›äº†å®Œæ•´çš„å‚è€ƒæ¡†æ¶ã€‚é€šè¿‡éµå¾ªè¿™äº›æœ€ä½³å®è·µï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é ã€å¯æ‰©å±•çš„æ‰¹å¤„ç†ç³»ç»Ÿã€‚
# SINQé‡åŒ–æŠ€æœ¯æŒ‡å—

## æ¦‚è¿°

SINQ (Sinkhorn-Normalized Quantization) æ˜¯ä¸€ç§æ–°é¢–ã€å¿«é€Ÿä¸”é«˜è´¨é‡çš„é‡åŒ–æ–¹æ³•ï¼Œä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡ï¼Œèƒ½å¤Ÿåœ¨ä¿æŒå‡†ç¡®æ€§çš„åŒæ—¶æ˜¾è‘—å‡å°æ¨¡å‹å¤§å°ã€‚SINQæ˜¯ä¸€ç§å…æ ¡å‡†çš„é‡åŒ–æŠ€æœ¯ï¼Œé€šè¿‡å¼•å…¥åŒç¼©æ”¾å› å­å’ŒåŸºäºSinkhorn-Knoppç®—æ³•çš„å½’ä¸€åŒ–æ–¹æ³•æ¥è§£å†³ä¼ ç»Ÿé‡åŒ–ä¸­çš„å¼‚å¸¸å€¼é—®é¢˜ã€‚

## æŠ€æœ¯åŸç†

### æ ¸å¿ƒåˆ›æ–°

#### 1. åŒç¼©æ”¾æœºåˆ¶

ä¼ ç»Ÿé‡åŒ–æ–¹æ³•æ¯ä¸ªæƒé‡ç»´åº¦åªä½¿ç”¨ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œè¿™ä½¿å¾—æ¨¡å‹å®¹æ˜“å—åˆ°å¼‚å¸¸å€¼çš„å½±å“ã€‚SINQé€šè¿‡å¼•å…¥**åŒç¼©æ”¾**æœºåˆ¶è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

- **è¡Œç¼©æ”¾å› å­**ï¼šä¸ºæ¯è¡Œæƒé‡è®¾ç½®ç‹¬ç«‹çš„ç¼©æ”¾å› å­
- **åˆ—ç¼©æ”¾å› å­**ï¼šä¸ºæ¯åˆ—æƒé‡è®¾ç½®ç‹¬ç«‹çš„ç¼©æ”¾å› å­

è¿™ç§çµæ´»æ€§é‡æ–°åˆ†é…äº†å¼‚å¸¸å€¼çš„å½±å“ï¼Œä½¿é‡åŒ–è¯¯å·®æ›´å°ä¸”æ›´å¹³è¡¡ã€‚

#### 2. Sinkhornå½’ä¸€åŒ–ä¼˜åŒ–

SINQä½¿ç”¨å—SinkhornçŸ©é˜µå½’ä¸€åŒ–å¯å‘çš„è¿­ä»£ç®—æ³•ï¼Œé‡æ–°ç¼©æ”¾è¡Œå’Œåˆ—ä»¥å¹³è¡¡å®ƒä»¬çš„æ–¹å·®ã€‚é€šè¿‡å‡å°‘æ•´ä½“çš„**çŸ©é˜µä¸å¹³è¡¡**ï¼Œæƒé‡å˜å¾—æ›´å®¹æ˜“é‡åŒ–ï¼Œå³ä½¿åœ¨æä½ä½å®½ä¸‹ä¹Ÿèƒ½ä¿æŒä¸€è‡´çš„é«˜ç²¾åº¦ã€‚

#### 3. æ›´å‡åŒ€çš„è¯¯å·®åˆ†å¸ƒ

ä¸æ ‡å‡†å•ç¼©æ”¾é‡åŒ–ç›¸æ¯”ï¼ŒSINQçš„è¯¯å·®åˆ†å¸ƒæ›´åŠ å‡åŒ€ä¸”ä¸é‚£ä¹ˆä¸¥é‡ï¼Œå³ä½¿åœ¨3ä½ç²¾åº¦ä¸‹ä¹Ÿèƒ½ä¿æŒæ¨¡å‹å‡†ç¡®æ€§ã€‚

## ä¸»è¦ç‰¹æ€§

### é‡åŒ–ç±»å‹æ”¯æŒ

- **å¯¹ç§°å’Œéå¯¹ç§°é‡åŒ–**ï¼šåŒæ—¶æ”¯æŒä¸¤ç§é‡åŒ–æ–¹å¼
- **NF4æ”¯æŒ**ï¼šæ”¯æŒéå‡åŒ€4ä½é‡åŒ–
- **å¤šç§ä½å®½**ï¼šæ”¯æŒ2ã€3ã€4ã€5ã€6ã€8ä½é‡åŒ–

### æ€§èƒ½ä¼˜åŠ¿

- **å…æ ¡å‡†**ï¼šä¸éœ€è¦æ ¡å‡†æ•°æ®é›†
- **å¿«é€Ÿé‡åŒ–**ï¼šæ¯”HQQå¿«çº¦2å€ï¼Œæ¯”AWQå¿«çº¦4å€
- **é«˜è´¨é‡**ï¼šåœ¨ç›¸åŒç²¾åº¦ä¸‹æä¾›æ›´å¥½çš„æ¨¡å‹æ€§èƒ½
- **æ¨¡å‹æ— å…³**ï¼šä¸éœ€è¦äº†è§£ç‰¹å®šçš„LLMæ¶æ„

## å®‰è£…

### åŸºç¡€å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/huawei-csl/SINQ.git
cd SINQ

# å®‰è£…ä¾èµ–
pip install -r req.txt

# å®‰è£…SINQ
pip install .
```

### å¯é€‰ä¾èµ–

```bash
# ç”¨äºä¿å­˜å’ŒåŠ è½½åˆ†ç‰‡safetensors
pip install safetensors
pip install gemlite==0.5.1.post1
```

## åŸºæœ¬ä½¿ç”¨

### æ¨¡å‹é‡åŒ–

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sinq.patch_model import AutoSINQHFModel
from sinq.sinqlinear import BaseQuantizeConfig

model_name = "Qwen/Qwen3-1.7B"
device = "cuda:0"

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# é…ç½®é‡åŒ–å‚æ•°
quant_cfg = BaseQuantizeConfig(
    nbits=4,            # é‡åŒ–ä½å®½
    group_size=64,      # ç»„å¤§å°
    tiling_mode="1D",   # å¹³é“ºç­–ç•¥
    method="sinq"       # é‡åŒ–æ–¹æ³•ï¼ˆ"asinq"ç”¨äºæ ¡å‡†ç‰ˆæœ¬ï¼‰
)

# æ‰§è¡Œé‡åŒ–
qmodel = AutoSINQHFModel.quantize_model(
    model,
    tokenizer=tokenizer,
    quant_config=quant_cfg,
    compute_dtype=torch.bfloat16,
    device=device
)
```

### ä¿å­˜å’ŒåŠ è½½æ¨¡å‹

```python
# ä¿å­˜ä¸ºåˆ†ç‰‡safetensorsæ ¼å¼
save_dir = "qwen3-1.7b-sinq-4bit"
AutoSINQHFModel.save_quantized_safetensors(
    qmodel,
    tokenizer,
    save_dir,
    verbose=True,
    max_shard_size="4GB",
)

# ä»åˆ†ç‰‡safetensorsåŠ è½½
tokenizer = AutoTokenizer.from_pretrained(save_dir)
device = "cuda:0"
qmodel = AutoSINQHFModel.from_quantized_safetensors(
    save_dir,
    device=device,
    compute_dtype=torch.bfloat16,
)
```

### æ¨ç†åŠ é€Ÿ

```python
# é¢„çƒ­ä»¥åˆå§‹åŒ–CUDAå›¾
_ = qmodel.forward(torch.tensor([[0]], device=device))

# ç¼–è¯‘ä»¥åŠ é€Ÿæ¨ç†
qmodel.forward = torch.compile(
    qmodel.forward,
    dynamic=True,
    fullgraph=False,
    backend="inductor",
    mode="reduce-overhead",
)
```

## é…ç½®å‚æ•°

### ä¸»è¦å‚æ•°

| å‚æ•° | æè¿° | é€‰é¡¹ | é»˜è®¤å€¼ |
|------|------|------|--------|
| `nbits` | æƒé‡é‡åŒ–ä½å®½ | 2, 3, 4, 5, 6, 8 | 4 |
| `tiling_mode` | æƒé‡çŸ©é˜µå¹³é“ºç­–ç•¥ | 1D, 2D | 1D |
| `group_size` | æ¯ä¸ªé‡åŒ–ç»„çš„æƒé‡æ•° | 64, 128 | 64 |
| `method` | é‡åŒ–æ–¹æ³• | sinq, asinq | sinq |

### æ–¹æ³•é€‰æ‹©

- **sinq**ï¼šå…æ ¡å‡†ç‰ˆæœ¬ï¼Œå¿«é€Ÿä¸”é«˜è´¨é‡
- **asinq**ï¼šæ ¡å‡†ç‰ˆæœ¬ï¼Œç»“åˆAWQæ ¡å‡†ä»¥è·å¾—æ›´é«˜ç²¾åº¦
- **sinq_nf4**ï¼šéå‡åŒ€4ä½é‡åŒ–ç‰ˆæœ¬

## æ€§èƒ½åŸºå‡†

### é‡åŒ–é€Ÿåº¦

- **Qwen3-14B**ï¼šçº¦21ç§’
- **DeepSeekV2.5-236B**ï¼šçº¦5åˆ†é’Ÿ

### å†…å­˜èŠ‚çœ

- **DeepSeekV2.5-236B**ï¼šä»~472GBå‡å°‘åˆ°~110GB
- **ç²¾åº¦æŸå¤±**ï¼šWikiText2å’ŒC4ä¸Š< 1 ppl

### ä¸å…¶ä»–æ–¹æ³•å¯¹æ¯”

| ç‰¹æ€§ | SINQ | HQQ | A-SINQ | AWQ |
|------|------|-----|--------|-----|
| æ ¡å‡† | å…æ ¡å‡† | å…æ ¡å‡† | æ ¡å‡† | æ ¡å‡† |
| é‡åŒ–ç±»å‹ | å¯¹ç§°&éå¯¹ç§° | ä»…éå¯¹ç§° | å¯¹ç§°&éå¯¹ç§° | å¯¹ç§°&éå¯¹ç§° |
| NF4æ”¯æŒ | æ˜¯ | å¦ | æ˜¯ | å¦ |
| é‡åŒ–é€Ÿåº¦ | æ¯”HQQå¿«2å€ | è¾ƒæ…¢ | æ¯”AWQå¿«4å€ | è¾ƒæ…¢ |
| æ¨¡å‹è´¨é‡ | æ›´é«˜ | è¾ƒä½ | æ›´é«˜ | è¾ƒä½ |

## é«˜çº§åŠŸèƒ½

### ä¸lm-evalæ¡†æ¶é›†æˆ

```python
from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM

# åŒ…è£…é‡åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
lm = HFLM(pretrained=qmodel, tokenizer=tokenizer, device=device)

# è¯„ä¼°
results = evaluator.simple_evaluate(
    model=lm,
    tasks=["lambada_openai"],
    device=device
)
```

### ä»Hugging Face HubåŠ è½½é¢„é‡åŒ–æ¨¡å‹

```python
import torch
from transformers import AutoTokenizer
from sinq.patch_model import AutoSINQHFModel

model_name = "huawei-csl/<model_name>"  # ä»é›†åˆä¸­é€‰æ‹©æ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cuda:0"

qmodel = AutoSINQHFModel.from_quantized_safetensors(
    model_name,
    device=device,
    compute_dtype=torch.bfloat16,
)
```

## ä¸å½“å‰é¡¹ç›®é›†æˆ

### ä¾èµ–é…ç½®

åœ¨`pyproject.toml`ä¸­æ·»åŠ SINQç›¸å…³ä¾èµ–ï¼š

```toml
[project.optional-dependencies]
quan = [
    "sinq @ git+https://github.com/huawei-csl/SINQ.git",
    "safetensors",
    "gemlite==0.5.1.post1",
]
```

### æ¨¡å‹åŠ è½½å™¨æ‰©å±•

```python
from sinq.patch_model import AutoSINQHFModel
from sinq.sinqlinear import BaseQuantizeConfig
from emb_model_provider.loaders.base_loader import BaseModelLoader

class SINQLoader(BaseModelLoader):
    def load_model(self):
        quant_cfg = BaseQuantizeConfig(
            nbits=4,
            group_size=64,
            tiling_mode="1D",
            method="sinq"
        )
        
        model = AutoSINQHFModel.quantize_model(
            self.model_name,
            quant_config=quant_cfg,
            device=self.get_device()
        )
        tokenizer = model.tokenizer
        return model, tokenizer
```

## æœªæ¥å‘å±•

### å³å°†æ¨å‡ºçš„åŠŸèƒ½

- ğŸ¤— ä¸Hugging Face Transformersé›†æˆ
- æ”¯æŒConv2Då±‚å’Œtimmæ¨¡å‹
- æ··åˆç²¾åº¦é‡åŒ–æ”¯æŒ
- vLLMã€SGLangå’Œllama.cppæ¡†æ¶æ”¯æŒ

### æŒç»­æ›´æ–°

- [2025/09/26] SINQè®ºæ–‡å‘å¸ƒ
- [2025/09/30] SINQ GitHubä»“åº“å…¬å¼€
- [2025/10/02] è®ºæ–‡è¢«Hugging Face Papersæ”¶å½•
- [2025/10/17] é¦–æ‰¹é¢„é‡åŒ–SINQæ¨¡å‹åœ¨Hugging Face Hubå‘å¸ƒ
- [2025/10/23] ä½¿ç”¨gemliteåç«¯å®ç°æ›´å¿«æ¨ç†

## æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡è¿è¡Œ**ï¼šç”±äºå†…æ ¸/å›¾ç¼–è¯‘ï¼Œé¦–æ¬¡è¿è¡Œä¼šè¾ƒæ…¢ï¼Œåç»­è¿è¡Œä¼šå¿«å¾—å¤š
2. **å†…å­˜è¦æ±‚**ï¼šé‡åŒ–è¿‡ç¨‹éœ€è¦è¶³å¤Ÿçš„GPUå†…å­˜
3. **å…¼å®¹æ€§**ï¼šç¡®ä¿PyTorchç‰ˆæœ¬å…¼å®¹
4. **æ¨¡å‹è´¨é‡**ï¼šè™½ç„¶SINQæä¾›é«˜è´¨é‡é‡åŒ–ï¼Œä½†ä»å»ºè®®åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè¿›è¡Œæµ‹è¯•

## å‚è€ƒèµ„æ–™

- [SINQ GitHub](https://github.com/huawei-csl/SINQ)
- [SINQè®ºæ–‡](https://arxiv.org/abs/2509.22944)
- [Hugging Face SINQé›†åˆ](https://huggingface.co/collections/huawei-csl/sinq)
- [Qwen3-Quantization](https://github.com/Efficient-ML/Qwen3-Quantization)
- [HQQ](https://github.com/mobiusml/hqq)
